---
title: "M5 Forecasting Uncertainty"
subtitle: "A Kaggle Competition"
author: "Stephen Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
    html_document: 
        toc: false
    bookdown::pdf_document2:
        number_sections: true
        keep_tex: true 
        toc: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggcorrplot)
knitr::opts_chunk$set(
    echo = TRUE, 
    message = FALSE, 
    warning = FALSE, 
    fig.align = "center", 
    fig.pos = "H"
)
```

# Overview 

### Goal 
This Kaggle competition asks us to forecast the distribution of Walmart sales for each item i.e. to predict, for each item in the dataset, a number of sales that correspond to a given quantile of the item's sales probability distribution.

### Approach 
An item is denoted at the store level. So we don't want to just forecast the sales probability distribution of, for example, Fuji apples, but for Fuji apples in one particular Wisconsin store on Monday, April 25th, 2017.

My first pass solution will be to calculate the emperical quantiles of an item's sales. There are a couple considerations using this approach: 

1. We don't have enough years worth of data to make a reasonable statement about an item's sales probability distribution using its own previous year over year sales. For example, we only have 5 years of data, which would give us 5 data points for a given item, at a given store, on a given day of the year. 
2. So we must aggregate. However we need to be careful about deciding to what level of aggregation. For example, we want to know if an item's sales are dramatically different month to month, or store to store, or even Sunday to Monday, before making a claim about the distribution with aggregated data. 

Therefore, this analysis will proceed as follows: 

1. Description of the raw data files, including the necessary cleaning and processing. 
2. Graphical descriptions of the data with various formats and different levels of aggregation. The purpose here is to use this information to make a reasonable assumption about how to aggregate an items historical sales distribution. Most of the graphs will be left to the Appendix at the end, for brevity.  
3. Calculate the emperical cummulative distribution function (cdf). 
4. Fit a logit model to the emperical cdf. 
5. Use the fitted logit model to make predictions at each relevant quantile. 

# Data
The data comes with four files. Below shows a description of each. 

```{r read-data}
base_dir <- getwd()
data_dir <- file.path(base_dir, "data")

calendar_fname         <- file.path(data_dir, "calendar.csv")
sales_validation_fname <- file.path(data_dir, "sales_train_validation.csv")
prices_fname           <- file.path(data_dir, "sell_prices.csv")
sample_fname           <- file.path(data_dir, "sample_submission.csv")
```

### Calendar
The data starts in 2011 and ends in 2016. Similarly, we can see which events are recorded to use in the analysis. 
```{r calendar}
calendar_data <- read_csv(calendar_fname) %>% arrange(date)
head(calendar_data)
```

```{r calendar-tail}
tail(calendar_data)
```

```{r events}
unique(calendar_data$event_type_1)
```

### Sales 
The sales data is in a very wide format with each row acting as it's own timeseries for a given item at a given store. We can see the dimmensions, head, and a view of the column ID's. 
```{r sales}
sales_data <- read_csv(sales_validation_fname)
dim(sales_data)
```

```{r sales-head}
head(sales_data)
```

```{r sales-items}
head(sales_data$id)
```

### Price
This gives us data on the price of a given item, in a given store, for a given week. 
```{r price}
prices_data <- read_csv(prices_fname)
head(prices_data)
```

### Submission 
We can see a sample of what is expected from a submission, including a zoom in to the `id` column to get a better idea of what to submit. 
```{r sample}
sample_data <- read_csv(sample_fname)
head(sample_data)
```

```{r submissions}
sample(unique(sample_data$id), 10)
```

### Combine
We want to combine the data into something useful. In this case, we want so-called "Tidy" data where each row is an observation and each column is a variable. We can do the following (but it requires a large-ish computer with at least 16GB of memory). For brevity, I only keep data after June 2013. This is arbitrary, but makes my laptop much happier. 

```{r reshape-data}
# clear space for the reshaped data
rm(list = c("calendar_data", "prices_data", "sales_data", "sample_data"))

# cutoff the early data so my computer stays happy, and becauase I'm not 
# convinced that 2012 sales of whatever really adds much relevant signal 
# that we don't get from more recent data. 
start_date <- as.Date("2013-06-01")

# we want to order the days of the week for future 'facet' based plotting 
days_of_week <- c(
    "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"
)

# convert month numbers to readable name 
month_mapping <- c(
        "January", "February", "March", "April", "May", "June", "July", "August", 
        "September", "October", "November", "December"
    )
month_int_to_name <- function(month_int, month_mapping) {
   return(month_mapping[month_int]) 
}

# because the data is so large, we read the data in without storing the 
# intermittent steps 
data <- read_csv(sales_validation_fname) %>% 
    pivot_longer(
        cols = -contains("id"), 
        names_to = "day", 
        values_to = "sales"
    ) %>% 
    left_join(
        read_csv(calendar_fname), 
        by = c("day" = "d")
    ) %>% 
    filter(
        date >= start_date
    ) %>% 
    left_join(
        read_csv(prices_fname), 
        by = c(
            "wm_yr_wk" = "wm_yr_wk", 
            "item_id" = "item_id", 
            "store_id" = "store_id"
        )
    ) %>% 
    filter(
        !is.na(sell_price)
    ) %>%
    mutate(
        weekday_ordered = factor(
            weekday, ordered = TRUE, levels = days_of_week
        ), 
        weekday = as_factor(weekday),
        event_type_1 = replace_na(event_type_1, "None"), 
        month_name = factor(
            sapply(month, month_int_to_name, month_mapping), ordered = TRUE, 
            levels = month_mapping
        )
    )

head(data)
```

# Graphs
Below we see histograms of item level sales differentiated by various (possibly) relevant variables. More aggregate graphs are in the Appendix at the end.

### Histograms - Daily Sales
This section gives us some intuition about how these item sales are distributed across various possible aggregators. 

#### Item by Weekday (randomly selected)

```{r histogram-item}
set.seed(271)                                    # set seed for reproducibility
selected_items <- sample(data$item_id, 7)        # get 5 "random" items 
data %>% 
    filter(
        item_id %in% selected_items
    ) %>% 
    group_by(
        date, item_id, weekday
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ggplot(aes(sales, ..scaled.., color = weekday)) + 
        geom_density() + 
        facet_wrap(~ item_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank(), 
            legend.title = element_blank()
        )
```

#### Item by Store 

```{r histogram-item-store}
data %>% 
    filter(
        item_id %in% selected_items
    ) %>% 
    group_by(
        date, item_id, store_id
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ggplot(aes(sales, ..scaled.., color = store_id)) + 
        geom_density() + 
        facet_wrap(~ item_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank(), 
            legend.title = element_blank()
        )
```

#### Item by Month 

```{r histogram-item-month}
data %>% 
    filter(
        item_id %in% selected_items
    ) %>% 
    group_by(
        date, item_id, month_name
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ggplot(aes(sales, ..scaled.., color = month_name)) + 
        geom_density() + 
        facet_wrap(~ item_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank(), 
            legend.title = element_blank()
        )
```

# Prediction 
To generate predictions, I do the following: 

1. Calculate the emperical cummulative distribution function for at item at some level of aggregation (in order to have enough data). More on this below. 
2. Fit a logit model to the cdf. 
3. Use the fitted model to make predictions for each quantile. 

### Aggregation
Based on the similarities and differences in the distributions shown above, as well as on the correlations seen in the appendix below, I use the following aggregation buckets: 

1. Months are grouped into quarters
   - Q1 are January, February, March 
   - Q2 are April, May, June 
   - Q3 are July, August, September 
   - Q4 art October, November, December 
2. Weekdays are groupoed into weekdays and weekends
   - Weekdays are Monday through Friday 
   - Weekends are Saturday and Sunday 
  
The following code does this for us: 
```{r aggregate}
filter_item_sales <- function(data, item_id, dow, m) {
    
    # storage 
    weekend <- c("Sunday", "Saturday")       # weekday and weekends? 
    q1      <- 1:3                           # which quarter? 
    q2      <- 4:6
    q3      <- 7:9
    q4      <- 10:12
    
    return(data %>%
        filter(
            id == !!item_id 
        ) %>% 
        filter(
            if (any(!!dow %in% weekend)) {
                weekday %in% weekend
            } else {
                !weekday %in% weekend
            }
        ) %>% 
        filter(
            if (any(m %in% q1)) {
                month %in% q1
            } else if (any(m %in% q2)) {
                month %in% q2
            } else if (any(m %in% q3)) {
                month %in% q3
            } else {
                month %in% q4
            }
        ) %>% 
        select(
            sales
        )
    )
}
```
   
### Emperical CDF 
The emperical cummulative distribution function is straightforward to calculate by ordering the realized sales for a given item (aggregated as described above), and normalizing the vector such that the last element is one (1). 

The following function receives the output of the function above and returns the cdf with the corresponding number of sales for that item: 
```{r ecdf}
calc_cdf <- function(item_sales) {
    if (length(item_sales) == 0) {
        return(
            tibble(
                cdf = seq(0, 1, length = 20), 
                sales = rep(0, length = 20)
            )
        )
    }
    
    sales <- sort(item_sales)
    cdf   <- 1:length(sales) / length(sales)
    
    data <- tibble(cdf = cdf, sales = sales)
    return(data)
}
```

### Logit predictions
Next we fit a logit to the sales and corresponding cdf found above.Ultimately, we want to input a quantile (x) and receive an output for the predicted number of sales at that quantile (y). Several notes about the code:

1. Since the logit is bound by zero (0) and one (1), we have to normalize the sales. 
2. Fit a logit model of sales on the emperical cdf. 
3. Make predictions on a quantile of interval 0.05 (0, 0.005, 0.01, ..., 1).
4. Rescale the predictions back to fit the observed sales data. I do something weird and totally not kosher at the end by forcing the tail of the distribution to catch the maximum observed number of sales. This happens because the logit minimizes the mean squared error, and just isn't always able to actually touch the observed maximum. Since it doesn't make any economic sense to think about the 99th percentile selling only 4.78 items, when maybe last week they actually sold 5, I force the tail of the distribution to stretch. 


```{r fit-logit}
calc_predictions <- function(ecdf_data, nsteps = 201) {
    
    # final quantile buckets 
    x    <- seq(0, 1, length = nsteps)
    
    # normalize sales to 0 / 1 make logit prediction
    max_sales <- max(ecdf_data$sales)
    if (max_sales == 0) {
        return(
            tibble(
                quantile = x, 
                sales_prediction = rep(0, nsteps)
            )
        )
    }
    
    cdf <- ecdf_data$cdf
    sales <- ecdf_data$sales / max_sales
    
    res <- glm(sales ~ cdf, family = binomial("logit"))
    
    # make zero one predictions 
    pred <- predict(res, list(cdf = x), type = "response")
    
    # rescale so that the 99th percentile is the max sales
    # i hate myself for doing this, but manually override the 
    # 98th+ percentile observations to rescale the tails to max
    max_pred               <- max(pred) # this is less than one, which we don't want
    if (max_pred > 0) {
        idx                    <- which(x > 0.98)[1]
        rescaled_pred_95th     <- pred[0: idx] * max_sales 
        rescaled_pred_99th     <- tail(pred, -idx) * max_sales / max_pred
        pred                   <- c(rescaled_pred_95th, rescaled_pred_99th)
    }
    return(
        tibble(
            quantile = x, 
            sales_prediction = pred
        )
    )
}
```

### Plot the fit
I know you must be as curious as I am to see how well this fits the data! 
```{r plot-fit}
plot_predictions <- function(observed_data, prediction_data) {
    cdf     <- observed_data$cdf 
    sales   <- observed_data$sales
    cdf_fit <- prediction_data$quantile 
    x       <- prediction_data$sales_prediction
    plot(cdf, sales)
    lines(cdf_fit, x, col = "steelblue", lty = 2)
}
```

### Calling function 
This calling function just acts as a wrapper for the above functions to simplify implementation at scale. 

```{r wrapper}
make_sales_prediction <- function(data, item_id, dow, m, graph = FALSE) {
    item_sales <- data %>% filter_item_sales(item_id, dow, m)
    ecdf_data <- item_sales$sales %>% calc_cdf()
    sales_predictions <- calc_predictions(ecdf_data)
    if (graph) {
        plot_predictions(ecdf_data, sales_predictions)
    }
    return (sales_predictions)
}
```

# Examples
We can see out how well this method fits the historic data visually. Note that this part still doesn't tell us about how well my aggregation assumptions were. 

### Weekday differences
We see how (and if!) the distribution changes for this item comparing a Monday in Q3 to a Monday in Q4.
```{r example-setup}
set.seed(981)
items   <- sample(data$id, 3) 

item    <- 1
day     <- "Monday"              
m       <- 7                     # Q3

prediction_data <- data %>% make_sales_prediction(items[item], day, m, graph = TRUE)
```

```{r example1}
item    <- 1
day     <- "Monday"              
month   <- 12                    # Q4

prediction_data <- data %>% make_sales_prediction(items[item], day, month, graph = TRUE)
```

### Quarterly differences
We see how (and if!) the distribution changes for this item comparing a Monday in Q2 to a Sunday in Q2.
```{r example2}
item    <- 2
day     <- "Monday"              # weekday
month   <- 4                     

prediction_data <- data %>% make_sales_prediction(items[item], day, month, graph = TRUE)
```

```{r example3}
item    <- 2
day     <- "Sunday"              # weekday
month   <- 4                     

prediction_data <- data %>% make_sales_prediction(items[item], day, month, graph = TRUE)
```

### All differences
We see how (and if!) the distribution changes for this item comparing a Monday in Q4 to a Sunday in Q3.
```{r example4}
item    <- 3
day     <- "Monday"              # weekday
month   <- 12                    # Q4

prediction_data <- data %>% make_sales_prediction(items[item], day, month, graph = TRUE)
```

```{r example5}
item    <- 3
day     <- "Sunday"              # weekend
month   <- 7                     # Q3

prediction_data <- data %>% make_sales_prediction(items[item], day, month, graph = TRUE)
```

### Quantile output 
We can see what the actual quantile output looks like, too! (for the above item).

```{r head}
prediction_data %>% head(10)
```

```{r tail}
prediction_data %>% tail(10)
```

# Appendix 

### Histograms 

#### By Department
This exercise is largely evidence of the central limit theorem (we shouldn't be too surprised that the histograms look failry normal since they represent the histogram of an aggregated random variable). The key take away here is to think about what type of volume these various departments are experiencing daily.

```{r histogram-dept}
data %>% 
    group_by(
        date, dept_id
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(x = sales, y = ..scaled..)) + 
        facet_wrap(~ dept_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```

#### By Weekday
The weekend days (Saturday and Sunday) see the highest volume. Monday through Friday are all vaguely similar in mean and variance. 

```{r histogram-weekday}
data %>% 
    group_by(
        wm_yr_wk, weekday_ordered
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(sales, ..scaled..)) + 
        facet_wrap(~ weekday_ordered, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```

#### By Store
Daily sales by store are quite different, as we would expect. 

```{r histogram-store}
data %>% 
    group_by(
        date, store_id
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(sales, ..scaled..)) + 
        facet_wrap(~ store_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```

#### By Month
In aggregate, monthly sales are extremely similar. This is most likely masking some significant differences in what items are being sold i.e. in Wisconsin, you may buy hiking gear in the summer, and approximately the same amount of snow gear in the winter. 

```{r histogram-month}
data %>% 
    group_by(
        date, month_name
    ) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(sales, ..scaled..)) + 
        facet_wrap(~ month_name, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```


### Correlation 

#### By Department
We see high correlation by department with the notable exception of HOBBIES_2.

```{r correlation-dept}
data %>% 
    group_by(date, dept_id) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    pivot_wider(
        id_cols = date, 
        names_from = dept_id, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- date) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Department"
    )

```

#### By Weekday
The most prominent correlation is Saturday and Sunday sales. Nothing looks negative. 

```{r correlation-weekday}
data %>% 
    group_by(wm_yr_wk, weekday_ordered) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    pivot_wider(
        id_cols = wm_yr_wk, 
        names_from = weekday_ordered, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- wm_yr_wk) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Weekday"
    )
```

#### By Store
Stores appear to be highly correlated overall in total daily sales volume, which is interesting, and a bit different than what I would have guessed. Since this is an aggregate though, it still may mask some item level differences e.g. it's way less likely that surf boards would sell well in Wisconsin, whereas they may sell well in a California summer.  

```{r correlation-store}
data %>% 
    group_by(date, store_id) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    pivot_wider(
        id_cols = date, 
        names_from = store_id, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- date) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Store"
    )
```

#### By Month
We notice some key differences here, notably July and December are relatively uncorrelated in sales volume. July and December appear to be involved in most of the uncorrelated relationships. 

```{r correlation-month}
data %>% 
    group_by(weekday, month_name) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    pivot_wider(
        id_cols = weekday, 
        names_from = month_name, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- weekday) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Month"
    )
```


### Timeseries 

#### By Department
We see that the department FOODS_3 and HOUSEHOLD_1 sales appear more volatile than the other departments. Also, we see a huge dip in every department on the same day: Christmas Day. 

```{r ts-dept}
data %>% 
    group_by(date, dept_id) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ggplot(
        aes(date, sales, color = dept_id)
        ) + 
        geom_line() + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank(),
            axis.title.x = element_blank(),
            legend.title = element_blank()
        )
```

#### By Weekday
Not much to see here. Note that the big dips are Christmas day. 

```{r ts-weekday}
data %>% 
    group_by(date, weekday_ordered) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ggplot(
        aes(date, sales)
        ) + 
        geom_line() + 
        facet_wrap(~ weekday_ordered, ncol = 1) +
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank(),
            axis.title.x = element_blank(),
            legend.title = element_blank()
        )
```

#### By Store
Stores may show some difference in sales, but nothing stands out too much. We'll revisit store level differences later. 

```{r ts-store}
data %>% 
    group_by(date, store_id) %>% 
    summarise(
        sales = sum(sales)
    ) %>% 
    ggplot(
        aes(date, sales, color = store_id)
        ) + 
        geom_line() + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank(),
            axis.title.x = element_blank(),
            legend.title = element_blank()
        )
```




