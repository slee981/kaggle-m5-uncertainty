---
title: "M5 Forecasting Uncertainty"
author: "Stephen Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
    html_document: 
        toc: false
    bookdown::pdf_document2:
        number_sections: true
        keep_tex: true 
        toc: false
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(
    echo = TRUE, 
    message = FALSE, 
    warning = FALSE, 
    fig.align = "center", 
    fig.pos = "H", 
    fig.height = 2, 
    fig.width = 7
)
```

## Read Data
The data comes with four files. Below shows a description of each. 

```{r read-data}
base_dir <- getwd()
data_dir <- file.path(base_dir, "data")

calendar_fname         <- file.path(data_dir, "calendar.csv")
sales_validation_fname <- file.path(data_dir, "sales_train_validation.csv")
prices_fname           <- file.path(data_dir, "sell_prices.csv")
sample_fname           <- file.path(data_dir, "sample_submission.csv")
```

### Calendar Data 
The data starts in 2011 and ends in 2016. Similarly, we can see which events are recorded to use in the analysis. 
```{r calendar}
calendar_data <- read_csv(calendar_fname) %>% arrange(date)
head(calendar_data)
```

```{r calendar-tail}
tail(calendar_data)
```

```{r events}
unique(calendar_data$event_type_1)
```

### Sales Data 
The sales data is in a very wide format with each row acting as it's own timeseries for a given item at a given store. We can see the dimmensions, head, and a view of the column ID's. 
```{r sales}
sales_data <- read_csv(sales_validation_fname)
dim(sales_data)
```

```{r sales-head}
head(sales_data)
```

```{r sales-items}
head(sales_data$id)
```

### Price Data
This gives us data on the price of a given item, in a given store, for a given week. 
```{r price}
prices_data <- read_csv(prices_fname)
head(prices_data)
```

### Sample Submission 
We can see a sample of what is expected from a submission, including a zoom in to the `id` column to get a better idea of what to submit. 
```{r sample}
sample_data <- read_csv(sample_fname)
head(sample_data)
```

```{r submissions}
sample(unique(sample_data$id), 10)
```

## Reshape 
We want to combine the data into something useful. In this case, we want so-called "Tidy" data where each row is an observation and each column is a variable. We can do the following (but it requires a large-ish computer with at least 16GB of memory). For brevity, I only keep data after June 2015. This somewhat arbitrary, but seems reasonable for a first pass since 2011 sales may say little about sales in 2016 or 2017 due to changes in consumer preferences, availability, etc. 

```{r reshape-data}
# clear space for the reshaped data
rm(list = c("calendar_data", "prices_data", "sales_data", "sample_data"))

# we want to order the days of the week for future 'facet' based plotting 
days_of_week <- c(
    "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"
)

# because the data is so large, we read the data in without storing the 
# intermittent steps 
data <- read_csv(sales_validation_fname) %>% 
    pivot_longer(
        cols = -contains("id"), 
        names_to = "day", 
        values_to = "sales"
    ) %>% 
    left_join(
        read_csv(calendar_fname), 
        by = c("day" = "d")
    ) %>% 
    filter(
        date > as.Date("2015-06-01")
    ) %>% 
    left_join(
        read_csv(prices_fname), 
        by = c(
            "wm_yr_wk" = "wm_yr_wk", 
            "item_id" = "item_id", 
            "store_id" = "store_id"
        )
    ) %>% 
    mutate(
        weekday = factor(
            weekday, ordered = TRUE, levels = days_of_week
        ), 
        event_type_1 = replace_na(event_type_1, "None"), 
        month = as_factor(month)
    )

head(data)
```

## View Data
Below I will show several view of the data, using several helpful aggregations. 

### Timeseries 

### Correlation 

### Histograms 


## Prediction 


