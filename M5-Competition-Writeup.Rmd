---
title: "M5 Forecasting Uncertainty"
author: "Stephen Lee"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
    html_document: 
        toc: false
    bookdown::pdf_document2:
        number_sections: true
        keep_tex: true 
        toc: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggcorrplot)
knitr::opts_chunk$set(
    echo = TRUE, 
    message = FALSE, 
    warning = FALSE, 
    fig.align = "center", 
    fig.pos = "H"
)
```

## Overview 
This Kaggle competition asks us to forecast the distribution of Walmart sales for each item i.e. to predict, for each item in the dataset, a number of sales that correspond to a given quantile.

## Read Data
The data comes with four files. Below shows a description of each. 

```{r read-data}
base_dir <- getwd()
data_dir <- file.path(base_dir, "data")

calendar_fname         <- file.path(data_dir, "calendar.csv")
sales_validation_fname <- file.path(data_dir, "sales_train_validation.csv")
prices_fname           <- file.path(data_dir, "sell_prices.csv")
sample_fname           <- file.path(data_dir, "sample_submission.csv")
```

### Calendar Data 
The data starts in 2011 and ends in 2016. Similarly, we can see which events are recorded to use in the analysis. 
```{r calendar}
calendar_data <- read_csv(calendar_fname) %>% arrange(date)
head(calendar_data)
```

```{r calendar-tail}
tail(calendar_data)
```

```{r events}
unique(calendar_data$event_type_1)
```

### Sales Data 
The sales data is in a very wide format with each row acting as it's own timeseries for a given item at a given store. We can see the dimmensions, head, and a view of the column ID's. 
```{r sales}
sales_data <- read_csv(sales_validation_fname)
dim(sales_data)
```

```{r sales-head}
head(sales_data)
```

```{r sales-items}
head(sales_data$id)
```

### Price Data
This gives us data on the price of a given item, in a given store, for a given week. 
```{r price}
prices_data <- read_csv(prices_fname)
head(prices_data)
```

### Sample Submission 
We can see a sample of what is expected from a submission, including a zoom in to the `id` column to get a better idea of what to submit. 
```{r sample}
sample_data <- read_csv(sample_fname)
head(sample_data)
```

```{r submissions}
sample(unique(sample_data$id), 10)
```

## Reshape 
We want to combine the data into something useful. In this case, we want so-called "Tidy" data where each row is an observation and each column is a variable. We can do the following (but it requires a large-ish computer with at least 16GB of memory). For brevity, I only keep data after June 2015. This somewhat arbitrary, but seems reasonable for a first pass since 2011 sales may say little about sales in 2016 due to changes in consumer preferences, availability, etc. 

```{r reshape-data}
# clear space for the reshaped data
rm(list = c("calendar_data", "prices_data", "sales_data", "sample_data"))

# we want to order the days of the week for future 'facet' based plotting 
days_of_week <- c(
    "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"
)

# because the data is so large, we read the data in without storing the 
# intermittent steps 
data <- read_csv(sales_validation_fname) %>% 
    pivot_longer(
        cols = -contains("id"), 
        names_to = "day", 
        values_to = "sales"
    ) %>% 
    left_join(
        read_csv(calendar_fname), 
        by = c("day" = "d")
    ) %>% 
    filter(
        date > as.Date("2015-06-01")
    ) %>% 
    left_join(
        read_csv(prices_fname), 
        by = c(
            "wm_yr_wk" = "wm_yr_wk", 
            "item_id" = "item_id", 
            "store_id" = "store_id"
        )
    ) %>% 
    filter(
        !is.na(sell_price)
    ) %>%
    mutate(
        weekday_ordered = factor(
            weekday, ordered = TRUE, levels = days_of_week
        ), 
        event_type_1 = replace_na(event_type_1, "None"), 
        month = as_factor(month)
    )

head(data)
```

## View Data
In order to make the problem more tractable, we want to aggregate our exploration into three useful buckets: 

1. by department, 
2. by day of the week, and 
3. by store. 

### Timeseries 
We can see several timeseries of sales aggregated to different buckets. 

#### By Department
```{r ts-dept}
data %>% 
    group_by(date, dept_id) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    ggplot(
        aes(date, sales)
        ) + 
        geom_line() + 
        facet_wrap(~ dept_id, ncol = 1) +
        theme_void()
```

#### By Weekday
```{r ts-weekday}
data %>% 
    group_by(date, weekday_ordered) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    ggplot(
        aes(date, sales)
        ) + 
        geom_line() + 
        facet_wrap(~ weekday_ordered, ncol = 1) +
        theme_void()
```

#### By Store
```{r ts-store}
data %>% 
    group_by(date, store_id) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    ggplot(
        aes(date, sales)
        ) + 
        geom_line() + 
        facet_wrap(~ store_id, ncol = 1) +
        theme_void()
```

### Correlation 

#### By Department
Aggregate the average sales per day for each department, and find the correlation between departments. 

```{r correlation-dept}
data %>% 
    group_by(date, dept_id) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    pivot_wider(
        id_cols = date, 
        names_from = dept_id, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- date) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Department"
    )

```

#### By Weekday
Aggregate the average sales per week for each weekday, and find the correlation between weekday sales. 

```{r correlation-weekday}
data %>% 
    group_by(wm_yr_wk, weekday_ordered) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    pivot_wider(
        id_cols = wm_yr_wk, 
        names_from = weekday_ordered, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- wm_yr_wk) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Weekday"
    )
```

#### By Store
Aggregate the average sales per day for each store, and find the correlation between departments. 

```{r correlation-store}
data %>% 
    group_by(date, store_id) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    pivot_wider(
        id_cols = date, 
        names_from = store_id, 
        values_from = sales
    ) %>% 
    ungroup() %>% 
    select(- date) %>%
    as.data.frame() %>% 
    cor(use = "pairwise.complete.obs") %>%
    ggcorrplot(
        legend.title = "", 
        outline.color = "white"
    ) +
    theme(
        plot.title = element_text(hjust = 0.5)
    ) +
    labs(
        title = "Correlation by Store"
    )
```

### Histograms 

#### By Department 

```{r histogram-dept}
data %>% 
    group_by(
        date, dept_id
    ) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(x = sales, y = ..scaled..)) + 
        facet_wrap(~ dept_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```

#### By Weekday

```{r histogram-weekday}
data %>% 
    group_by(
        wm_yr_wk, weekday_ordered
    ) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(sales, ..scaled..)) + 
        facet_wrap(~ weekday_ordered, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```

#### By Store

```{r histogram-store}
data %>% 
    group_by(
        date, store_id
    ) %>% 
    summarise(
        sales = mean(sales)
    ) %>% 
    ungroup() %>% 
    ggplot() + 
        geom_density(aes(sales, ..scaled..)) + 
        facet_wrap(~ store_id, ncol = 1) + 
        theme_minimal() + 
        theme(
            axis.text.y = element_blank(), 
            axis.ticks.y = element_blank(), 
            axis.title.y = element_blank()
        )
```

## Prediction 


